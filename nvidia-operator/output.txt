1. Creating a new Namwspace.

##################################################

apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-gpu-operator

# oc create -f nvidia-gpu-operator-namespace.yaml

##################################################


2. Creating a new Operator Group.

##################################################

apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator-group
  namespace: nvidia-gpu-operator
spec:
 targetNamespaces:
 - nvidia-gpu-operator

# oc create -f nvidia-gpu-operatorgroup.yaml

###################################################

3. Checking a version of the default Channel.

#######################################################################################################################################

# oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}'

v23.6

#######################################################################################################################################

4. Defining settings that will save the defaultChannel version and extracting the value of the existing channels in the current CSV.

###############################################################################################################################################################

# CHANNEL=v22.9
# $ oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == "'$CHANNEL'") | .currentCSV'

gpu-operator-certified.v22.9.1

###############################################################################################################################################################

5. Creating a new Subscription

####################################################################################

apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gpu-operator-certified
  namespace: nvidia-gpu-operator
spec:
  channel: "v22.9"
  installPlanApproval: Manual
  name: gpu-operator-certified
  source: certified-operators
  sourceNamespace: openshift-marketplace
  startingCSV: "gpu-operator-certified.v22.9.1"

# oc create -f nvidia-gpu-subscription.yaml

# oc get subscription -n nvidia-gpu-operator   
                                                                     
NAME                     PACKAGE                  SOURCE                CHANNEL

gpu-operator-certified   gpu-operator-certified   certified-operators   v22.9

####################################################################################

6. Checking that an install plan has been created

#############################################################################

# oc get installplan -n nvidia-gpu-operator

NAME            CSV                              APPROVAL   APPROVED

install-cqc5q   gpu-operator-certified.v22.9.2   Manual     false
install-mp9qr   gpu-operator-certified.v22.9.1   Manual     true

#############################################################################

7. Setting the Install plan variable and confirming the requested Install plan

#####################################################################################################

# INSTALL_PLAN=$(oc get installplan -n nvidia-gpu-operator -oname)
# oc patch $INSTALL_PLAN -n nvidia-gpu-operator --type merge --patch '{"spec":{"approved":true }}' 
                 
installplan.operators.coreos.com/install-p8t2k patched

# oc get installplan -n nvidia-gpu-operator   

NAME            CSV                              APPROVAL   APPROVED

install-p8t2k   gpu-operator-certified.v22.9.1   Manual     true

#####################################################################################################

8. Creating Cluster Policy.

###########################################################################################################################################################

# oc get csv -n nvidia-gpu-operator gpu-operator-certified.v22.9.2 -o jsonpath='{.metadata.annotations.alm-examples}' |  jq '.[0]' > clusterpolicy.json 


{
  "apiVersion": "nvidia.com/v1",
  "kind": "ClusterPolicy",
  "metadata": {
    "name": "gpu-cluster-policy"
  },
  "spec": {
    "operator": {
      "defaultRuntime": "crio",
      "use_ocp_driver_toolkit": true,
      "initContainer": {}
    },
    "sandboxWorkloads": {
      "enabled": false,
      "defaultWorkload": "container"
    },
    "driver": {
      "enabled": true,
      "repoConfig": {
        "configMapName": ""
      },
      "certConfig": {
        "name": ""
      },
      "licensingConfig": {
        "nlsEnabled": false,
        "configMapName": ""
      },
      "virtualTopology": {
        "config": ""
      },
      "kernelModuleConfig": {
        "name": ""
      }
    },
    "dcgmExporter": {
      "enabled": true,
      "config": {
        "name": ""
      },
      "serviceMonitor": {
        "enabled": true
      }
    },
    "dcgm": {
      "enabled": true
    },
    "daemonsets": {
      "updateStrategy": "RollingUpdate",
      "rollingUpdate": {
        "maxUnavailable": "1"
      }
    },
    "devicePlugin": {
      "enabled": true,
      "config": {
        "name": "",
        "default": ""
      }
    },
    "gfd": {
      "enabled": true
    },
    "migManager": {
      "enabled": true
    },
    "nodeStatusExporter": {
      "enabled": true
    },
    "mig": {
      "strategy": "single"
    },
    "toolkit": {
      "enabled": true
    },
    "validator": {
      "plugin": {
        "env": [
          {
            "name": "WITH_WORKLOAD",
            "value": "true"
          }
        ]
      }
    },
    "vgpuManager": {
      "enabled": false
    },
    "vgpuDeviceManager": {
      "enabled": true
    },
    "sandboxDevicePlugin": {
      "enabled": true
    },
    "vfioManager": {
      "enabled": true
    }
  }
}

# oc apply -f clusterpolicy.json 
                                                                                   
clusterpolicy.nvidia.com/gpu-cluster-policy created

###########################################################################################################################################################

9. Running a simple GPU application.

############################################################

# cat << EOF | oc create -f -                                                                                                                           
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vectoradd
spec:
 restartPolicy: OnFailure
 containers:
 - name: cuda-vectoradd
   image: "nvidia/samples:vectoradd-cuda11.2.1"
   resources:
     limits:
       nvidia.com/gpu: 1
EOF



# oc logs cuda-vectoradd           

                                                                                                                     
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done

############################################################


10. Getting information about the GPU

#############################################################################################################################

# oc project nvidia-gpu-operator 

# oc get pod -o wide -l openshift.driver-toolkit=true    
                                                                                                 
NAME                                                  READY   STATUS    RESTARTS   AGE   IP     
        
nvidia-driver-daemonset-410.84.202205191234-0-s29bv   2/2     Running   0          18m   10.131.1.203   

NODE                                     NOMINATED NODE   READINESS GATES

upgradedemo5-jt6xj-worker-westus-m2kzn   <none>           <none>


NAME                                                  READY   STATUS    RESTARTS   AGE   IP             

nvidia-driver-daemonset-410.84.202205191234-0-zqjfz   2/2     Running   0          22m   10.128.3.110

NODE                                     NOMINATED NODE   READINESS GATES

upgradedemo5-jt6xj-worker-westus-h9qh4   <none>           <none>



# oc exec -it nvidia-driver-daemonset-410.84.202205191234-0-s29bv -- nvidia-smi 
                                    
Defaulted container "nvidia-driver-ctr" out of: nvidia-driver-ctr, openshift-driver-toolkit-ctr, k8s-driver-manager (init)
Sat Sep 30 14:34:19 2023    
   
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                  Off |
| N/A   28C    P0    24W / 250W |      0MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  On   | 00000002:00:00.0 Off |                  Off |
| N/A   26C    P0    25W / 250W |      0MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+



# oc exec -it nvidia-driver-daemonset-410.84.202205191234-0-zqjfz -- nvidia-smi  
                                   
Defaulted container "nvidia-driver-ctr" out of: nvidia-driver-ctr, openshift-driver-toolkit-ctr, k8s-driver-manager (init)
Sat Sep 30 14:35:28 2023   

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                  Off |
| N/A   29C    P0    24W / 250W |      0MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


#############################################################################################################################
